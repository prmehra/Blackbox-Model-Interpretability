# Evauating the use of Decision Trees in Enhancing Blackbox Model Interpretability

######Research Paper Abstract
Machine learning algorithms have the potential to significantly enhance decision making processes in highly impactful fields, ranging from health care to criminal justice sentencing. Owing of the significance and intricate nature of the decisions that are often made in these fields, it is important for the experts using these algorithms to have a concrete understanding of how they work, and why they make the predictions they do. Unfortunately, some of the algorithms with the highest degrees of accuracy, offer zero levels of interpretability. Such models are referred to as being ‘blackbox’ in the literature. Recently there has been much interest in finding mechanisms to interpret these blackbox models, as it is generally believed that there is much value in being able to do so. In this paper, we explore the potential that decision trees have in enhancing blackbox model interpretability. To do so, we implement an evaluate an algorithm called Model Extraction — proposed by researchers at Stanford University — which aims to construct interpretable approximations of blackbox models. We develop heuristics and implement methods to evaluate the potential that Model Extraction has in enhancing blackbox model interpretability. Subsequently, we propose a variant of the algorithm which appears to perform better than the original algorithm, when implemented and evaluated with real world breast cancer data.
